
# coding: utf-8

# In[116]:


# TODO: package pythonutil scripts and Build\ Figures.py into one .py? Just cat them maybe perchance

get_ipython().system('jupyter nbconvert --to script Build\\ Figures.ipynb')

# Example command line usage:
#   $ cat bhl.nq | ipython Build\ Figures.py BHL
# which will save results to ./bhl-analysis/ and annotated figures will be titled "BHL"


# In[2]:


includeLegend = False
includeTitle = False
figureDpi = 300


# **BEWARE** - these scripts may require a LOT of memory and take quite a long time to complete

# In[3]:


INTERACTIVE = (get_ipython().__class__.__name__ == "ZMQInteractiveShell")
print("Interactive mode: " + str(INTERACTIVE))


# In[4]:


import sys

import numpy as np
import pandas as pd
from enum import Enum

from pythonutil.patterns import statement_pattern
from pythonutil.terms import *
from pythonutil.prestongraph import *


# ### Get provenance log
# 
# On a large preston.acis.ufl.edu observatory it can take a while to run ```preston ls```, so we store its output at a temporary location
# ```shell
# preston ls --remote http://preston.acis.ufl.edu > tmp/nquads
# ```

# ### Index the provenance logs

# In[5]:


class Observatory:
    def __init__(self, name, dataPath=None, outputPath=None):
        self.name = name
        self.dataPath = dataPath
        self.index = None
        
        if outputPath is not None:
            self.outputPath = outputPath + "/"
        else:
            self.outputPath = "./" + name.lower().replace(" ", "-") + "-analysis/"


# In[6]:


indexTerms = {
    DESCRIPTION,
    HAS_QUALIFIED_GENERATION,
    STARTED_AT_TIME,
    USED,
    WAS_INFLUENCED_BY, # changed from "ACTIVITY" to distinguish between "activity" and "Activity"
    WAS_INFORMED_BY
}

def MakeIndexFromLogs(paths):
    #import sys
    #import io
    index = Index()
    global nQuad
    global line

    # Parse the provenance log into a list of n-quads
    if paths == None:
        print("Reading from stdin...")
        for line in sys.stdin:
            index.Ingest(str(line))
    else:
        if type(paths) == str:
            paths = [paths]

        for path in paths:
            print("Reading from %s..." % path)
            with open(path) as file:
                for line in file:
                    nQuad = statement_pattern.match(str(line))
                    if "predicate" in nQuad.groupdict() and nQuad["predicate"] in indexTerms:
                        index.Ingest(str(line))

    return index


# In[7]:


observatories = []


# In[8]:


# observatories.append(Observatory("iDigBio", "../zenodo_deeplinker/idigbio.nq"))


# In[9]:


# observatories.append(Observatory("GBIF", "../zenodo_deeplinker/gbif.nq"))


# In[10]:


# observatories.append(Observatory("DataONE", "../zenodo_dataone/nquads"))


# In[11]:


observatories.append(Observatory("BHL", "./bhl.nq"))


# In[12]:


# observatories.append(Observatory("Deeplinker", "../zenodo_deeplinker/nquads"))


# In[13]:


# observatories.append(Observatory("GNV", "./gnv.nq"))


# In[14]:


# observatories.append(Observatory("All", [x.dataPath for x in observatories]))


# In[15]:


# If not in Jupyter, use stdin (dataPath=None) as input
if not INTERACTIVE:
    name = sys.argv[1] if len(sys.argv) > 1 else "Network"
    output_path = sys.argv[2] if len(sys.argv) > 2 else None
    
    observatories = [
        Observatory(name=name, dataPath=None, outputPath=output_path)
    ]

    print("Name: " + observatories[0].name)
    print("Output: " + observatories[0].outputPath)


# In[16]:


get_ipython().run_cell_magic('time', '', '\nimport os\n\nfor observatory in observatories:\n    if observatory.index is not None:\n        print("Skipping %s; already loaded" % observatory.name)\n        continue\n\n    # Prepare a place to store results\n    try:\n        os.mkdir(observatory.outputPath)\n    except OSError:\n        pass\n\n    # Make an index\n    print("Reading %s..." % observatory.name)\n    %time observatory.index = MakeIndexFromLogs(observatory.dataPath)\n    print("Done reading %s\\n" % observatory.name)')


# In[17]:


observatory = observatories[0]
observatoryName = observatory.name
fullIndex = observatory.index
outputPath = observatory.outputPath
dataPath = observatory.dataPath
print("Using network \"%s\"" % observatoryName)
print("Saving output in " + outputPath)

if fullIndex == None:
    print("Uh oh! Failed to parse input. Aborting")
    exit()


# In[18]:


fullIndex.verbLookup


# In[19]:


for x in fullIndex.verbLookup[STARTED_AT_TIME].statements: print(x)


# In[20]:


print("{:,}".format(len(fullIndex.nodes)), "nodes")


# In[21]:


# formats = set()

# for x in fullIndex.verbLookup["http://purl.org/dc/elements/1.1/format"].triples:
#     formats.add(x.object)

# for x in formats: print(x)


# In[22]:


for x in fullIndex.verbs: print(x)


# In[23]:


def PrintNodeSubTree(node, maxDepth=1, root=True):
    if root:
        for x in node.inwardStatements:
            print(x)

    if maxDepth > 0:
        for x in node.outwardStatements:
            print(x)
            PrintNodeSubTree(x.object, maxDepth - 1, False)

def Peek(nativeSet):
    if len(nativeSet) == 0:
        return None
    else:
        item = nativeSet.pop()
        nativeSet.add(item)
        return item

def ContentIsMissing(content):
    return str(content).startswith("<http")


# ### Get crawl UUIDs and sort them by date

# In[24]:


class CrawlMeta:
    def __init__(self, position, date):
        # Properties
        self.position = position
        self.date = date


# In[25]:


fullIndex.verbLookup[STARTED_AT_TIME].statements


# In[26]:


checkDescription = (DESCRIPTION in fullIndex.verbLookup)
startDate = "\"2019-03-01"
endDate = "\"2019-10-1"

crawlDates = []
for statement in fullIndex.verbLookup[STARTED_AT_TIME].statements:
    crawlUuid = statement.subject
    crawlDate = statement.object

    if crawlDate < startDate:
        print("Ignoring early crawl log %s (\"%s\")" % (str(crawlUuid), str(crawlDate)))
    else:
        if checkDescription:
            descriptionStatement = Peek(crawlUuid.outwardStatements & fullIndex.verbLookup[DESCRIPTION].statements)
            if descriptionStatement is None or descriptionStatement.object == "\"A crawl event that discovers biodiversity archives.\"@en":
                crawlDates.append((crawlUuid, crawlDate))
            else:
                print("Ignoring non-crawl log %s (\"%s\")" % (str(crawlUuid), str(descriptionStatement.object)))
        else:
            crawlDates.append((crawlUuid, crawlDate))
numCrawls = len(crawlDates)

crawlDates.sort(key=lambda x: x[1])

print()
# Assign some helpful metadata to each crawl
crawlMetaLookup = dict()
for i, crawlDate in enumerate(crawlDates):
    # Tag crawl nodes with their chronological position in the preston history
    crawlMeta = CrawlMeta(
        position=i,
        date=crawlDate[1]
    )

    crawlMetaLookup[crawlDate[0]] = crawlMeta

    print("%s\t%s\t%d" % (crawlDate[0], crawlMeta.date, crawlMeta.position))


# ### Build a history for each URL

# In[81]:


class Status(Enum):
    UNKNOWN             = 0    # Did not check for content
    FIRST_CONTENT       = 1    # Returned content for the first time
    SAME_CONTENT        = 2    # Returned the same content as the last successful query
    CHANGED_CONTENT     = 3    # Returned new content
    OLD_CONTENT         = 4    # Returned previously seen content that is different from the previous successful data
    BECAME_UNRESPONSIVE = 5    # Failed to return content after a successful query
    STILL_UNRESPONSIVE  = 6    # Failed to return content again
    ERROR               = 7    # Returned malformed content

class UrlLifetime:
    def __init__(self, numCrawls):
        self.statuses = [Status.UNKNOWN] * numCrawls
        self.contents = [None] * numCrawls

        self.firstCrawlPosition = None
        self.lastCrawlPosition = None
        self.lastKnownStatus = Status.UNKNOWN
        self.firstResponsePosition = None
        self.firstChangePosition = None
        self.firstBreakPosition = None

        self.numResolves = 0
        self.numBreaks = 0
        self.numContents = 0
        self.numContentChanges = 0

class ContentLifetime:
    def __init__(self, numCrawls):
        self.firstCrawlPosition = None
        self.lastCrawlPosition = None


# ### Collect the contents seen over the course of each URL's lifetime

# In[82]:


wasInfluencedByAndWasInformedByStatements = set()

if WAS_INFLUENCED_BY in fullIndex.verbLookup:
    wasInfluencedByAndWasInformedByStatements |= fullIndex.verbLookup[WAS_INFLUENCED_BY].statements
if WAS_INFORMED_BY in fullIndex.verbLookup:
    wasInfluencedByAndWasInformedByStatements |= fullIndex.verbLookup[WAS_INFORMED_BY].statements


# In[83]:


get_ipython().run_cell_magic('time', '', '\nnumGenerationsIgnored = 0\n\ncontentLifetimes = dict()\nurlLifetimes = dict()\nfor x in fullIndex.verbLookup[HAS_QUALIFIED_GENERATION].statements:\n    qualGen = x.object\n\n    # Get content\n    contentTriple = Peek(qualGen.inwardStatements & fullIndex.verbLookup[HAS_QUALIFIED_GENERATION].statements)\n    content = contentTriple.subject\n\n    # Get URL\n    urlTriple = Peek(qualGen.outwardStatements & fullIndex.verbLookup[USED].statements)\n    url = urlTriple.object\n\n    # Get crawl\n    crawlTriple = Peek(qualGen.outwardStatements & wasInfluencedByAndWasInformedByStatements)\n    crawl = crawlTriple.object\n    \n    # Skip generations from ignored crawls\n    if crawl not in crawlMetaLookup:\n        numGenerationsIgnored += 1\n        continue\n\n    crawlMeta = crawlMetaLookup[crawl]\n\n    # Fill in URL lifetime data\n    if url in urlLifetimes:\n        lifetime = urlLifetimes[url]\n    else:\n        lifetime = UrlLifetime(numCrawls)\n        urlLifetimes[url] = lifetime\n\n    # if lifetime.firstCrawlPosition == None or crawlMeta.position < lifetime.firstCrawlPosition:\n    #     lifetime.firstCrawlPosition = crawlMeta.position\n\n    # if lifetime.lastCrawlPosition == None or crawlMeta.position > lifetime.lastCrawlPosition:\n    #     lifetime.lastCrawlPosition = crawlMeta.position\n\n    lifetime.contents[crawlMeta.position] = content\n    \n    # Fill in content lifetime data\n    if not ContentIsMissing(content):\n        if content in contentLifetimes:\n            lifetime = contentLifetimes[content]\n        else:\n            lifetime = ContentLifetime(numCrawls)\n            contentLifetimes[content] = lifetime\n        \n        if lifetime.firstCrawlPosition == None or crawlMeta.position < lifetime.firstCrawlPosition:\n            lifetime.firstCrawlPosition = crawlMeta.position\n\n        if lifetime.lastCrawlPosition == None or crawlMeta.position > lifetime.lastCrawlPosition:\n            lifetime.lastCrawlPosition = crawlMeta.position\n\ntotalNumUrls = len(urlLifetimes)\nprint("{:,}".format(numGenerationsIgnored), "generations ignored")\nprint("{:,}".format(totalNumUrls), "urls processed")')


# ### Assign a status for each stage of each URL's lifetime

# There's some ambiguity in the status of a URL's first query, so we make some assumptions:
# * If unresolved, consider it BECAME_UNRESOLVED rather than STILL_UNRESOLVED
# * If resolved, consider it FIRST_CONTENT rather than SAME_CONTENT, OLD_CONTENT, or CHANGED_CONTENT

# In[85]:


get_ipython().run_cell_magic('time', '', '\nfor url, lifetime in urlLifetimes.items():\n    wasAlive = True\n    mostRecentContent = None\n    for i, content in enumerate(lifetime.contents):\n\n        if content:\n            if ContentIsMissing(content):\n        # Became unresponsive\n                if wasAlive:\n                    status = Status.BECAME_UNRESPONSIVE\n                    lifetime.numBreaks += 1\n\n        # Still unresponsive\n                else:\n                    status = Status.STILL_UNRESPONSIVE\n\n                wasAlive = False\n            \n                if lifetime.firstBreakPosition is None:\n                    lifetime.firstBreakPosition = i\n\n        # First content\n            else:\n                if mostRecentContent == None:\n                    status = Status.FIRST_CONTENT\n                    mostRecentContent = content\n                    lifetime.numContents += 1\n                    lifetime.firstResponsePosition = i\n\n        # Same content\n                elif content == mostRecentContent:\n                    status = Status.SAME_CONTENT\n\n                else:\n        # Old content\n                    if content in lifetime.contents[0:i]:\n                        status = Status.OLD_CONTENT\n\n        # Changed content\n                    else:\n                        status = Status.CHANGED_CONTENT\n                        if lifetime.firstChangePosition is None:\n                            lifetime.firstChangePosition = i\n                        lifetime.numContents += 1\n\n                    mostRecentContent = content\n                    lifetime.numContentChanges += 1\n\n                wasAlive = True\n                lifetime.numResolves += 1\n\n            lifetime.lastKnownStatus = status\n\n            if lifetime.firstCrawlPosition is None:\n                lifetime.firstCrawlPosition = i\n            lifetime.lastCrawlPosition = i\n\n        # Unknown\n        else:\n            status = Status.UNKNOWN\n\n        lifetime.statuses[i] = status')


# In[86]:


textUrlLifetimes = dict()
for url, lifetime in urlLifetimes.items():
    textContents = [None] * numCrawls
    for i, content in enumerate(lifetime.contents):
        textContents[i] = str(content)
    textUrlLifetimes[str(url)] = textContents

np.save(outputPath + "url-lifetimes", textUrlLifetimes)


# In[87]:


print("Lifetime for %s\n" % url)
print("\n".join(["%d:\t%s\t%s" % (i, lifetime.statuses[i], lifetime.contents[i]) for i in range(numCrawls)]))


# ## Build figures

# ### New, modified, lost content per crawl

# In[88]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rc
import datetime
from itertools import cycle, islice

# Only do this in Jupyter Notebook
if INTERACTIVE:
    get_ipython().run_line_magic('matplotlib', 'inline')


# In[91]:


crawlStatusTotals = [(dict([(status, 0) for status in Status])) for crawlDate in crawlDates]
# for crawl in range(numCrawls):
#     crawlStatusTotals[crawl] = 

# crawlStatusTotals = dict()


for _, lifetime in urlLifetimes.items():
    for i, status in enumerate(lifetime.statuses):
        crawlStatusTotals[i][status] += 1

# crawl_status_totals_df = pd.DataFrame(
#     index   = [datetime.datetime.strptime(str(x[1]), "%Y-%m-%dT%H:%M:%S.%fZ") for x in crawlDates],
#     columns = [x for x in Status],
#     data    = ,
#     dtype   = int
# )

crawl_status_totals_df = pd.DataFrame(
    index = [datetime.datetime.strptime(str(x[1]), "\"%Y-%m-%dT%H:%M:%S.%fZ\"^^<http://www.w3.org/2001/XMLSchema#dateTime>") for x in crawlDates],
    data = crawlStatusTotals
)

crawl_status_totals_df.to_csv(outputPath + "crawl-status-totals-df")
crawl_status_totals_df.transpose()


# In[92]:


col_hex = {
    "blue"      : "#1f77b4",
    "orange"    : "#ff7f0e",
    "green"     : "#2ca02c",
    "red"       : "#d62728",
    "purple"    : "#9467bd",
    "brown"     : "#8c564b",
    "pink"      : "#e377c2",
    "gray"      : "#7f7f7f",
    "yellow"    : "#bcbd22",
    "teal"      : "#17becf",
    
    "bright green"  : "#00ff00",
    "bright red"    : "#ff0000",
    "green yellow"  : "#9ACD32",
    "olive"         : "#808000"
}

fig_df = crawl_status_totals_df[[
    Status.SAME_CONTENT,
    Status.FIRST_CONTENT,
    Status.OLD_CONTENT,
    Status.CHANGED_CONTENT,
    Status.STILL_UNRESPONSIVE,
    Status.BECAME_UNRESPONSIVE,
    Status.UNKNOWN,
    Status.ERROR
]]

status_colors = {
    Status.UNKNOWN             : "gray",
    Status.FIRST_CONTENT       : "bright green",
    Status.SAME_CONTENT        : "green",
    Status.CHANGED_CONTENT     : "yellow",
    Status.OLD_CONTENT         : "olive",
    Status.BECAME_UNRESPONSIVE : "bright red",
    Status.STILL_UNRESPONSIVE  : "red",
    Status.ERROR               : "purple",
}

status_color_map = list(islice(cycle([col_hex[status_colors[x]] for x in fig_df.columns]), None, 256))

rc("text", usetex=False)
rc("savefig", format="png", dpi=figureDpi)
rc("font", size=14, family="DejaVu Sans")


# In[93]:


figureTitle = observatoryName + ": Stacked URL status counts per crawl"
outputFile = outputPath + "stacked-query-status-counts-per-crawl"

ax = fig_df.plot(
    kind="bar",
    stacked=True,
    width=.95,
    color=status_color_map,
    figsize=(15, 5),
    legend=False
);

# Bare
plt.savefig(outputFile, dpi=figureDpi);

# Annotated
plt.title(figureTitle)
plt.legend()
plt.savefig(outputFile[:-4] + "-annotated.png", dpi=figureDpi);


# In[94]:


figureTitle = observatoryName + ": Stacked URL Status Counts Over Time (Stepped)"
outputFile = outputPath + "stacked-query-status-counts-over-time"

fig = plt.figure(figsize=(16, 5))
ax = fig.add_subplot(111)

# Fill in space between lines
x = fig_df.index.append(pd.Index([datetime.datetime.now()]))
y1 = pd.Series({ q : 0 for q in x })

columns = fig_df.columns
n = len(columns)
for i in range(0, n):
    y2 = y1 + fig_df[columns[i]] + 0
    ax.fill_between(x, y1, y2, step="post", color=status_color_map[i])
    y1 = y2

# Bare
plt.savefig(outputFile, dpi=figureDpi);

# Annotated
plt.title(figureTitle)
plt.savefig(outputFile[:-4] + "-annotated.png", dpi=figureDpi);


# In[95]:


crawl_status_totals_df


# In[96]:


crawl_health_df = pd.DataFrame(index=crawl_status_totals_df.index)

crawl_health_df["Available URLs"] = crawl_status_totals_df[[
    Status.FIRST_CONTENT,
    Status.SAME_CONTENT,
    Status.CHANGED_CONTENT,
    Status.OLD_CONTENT
]].sum(axis=1)

crawl_health_df["Stable URLs"] = crawl_status_totals_df[[
    Status.FIRST_CONTENT,
    Status.SAME_CONTENT,
    Status.CHANGED_CONTENT,
    Status.OLD_CONTENT
]].sum(axis=1)

crawl_health_df


# ## Running URL and content totals

# In[98]:


crawlUrlTotals = [0] * numCrawls
crawlDatasetTotals = [0] * numCrawls
crawlAbandonedTotals = [0] * numCrawls
crawlFirstResponseTotals = [0] * numCrawls
crawlFirstBreakTotals = [0] * numCrawls
crawlFirstChangeTotals = [0] * numCrawls
crawlFirstUnreliableTotals = [0] * numCrawls

numUnresponsive = 0
numResponsive = 0
numStable = 0
numReliable = 0
numAbandoned = 0

totalNumResolves = 0
totalNumBreaks = 0
totalNumContents = 0
totalNumContentChanges = 0
maxNumContentChanges = 0

maxNumBreaks   = totalNumUrls * (np.ceil(numCrawls / 2)) # Round up; if the responsiveness is [0, 1, 0] across three crawls, there are at most two breaks
maxNumContents = totalNumUrls * (numCrawls)

breakCounts = []
contentChangeCounts = []

for _, lifetime in urlLifetimes.items():
    #if lifetime.firstCrawlPosition:
    crawlUrlTotals[lifetime.firstCrawlPosition] += 1

    totalNumResolves += lifetime.numResolves
    totalNumBreaks += lifetime.numBreaks
    totalNumContents += lifetime.numContents
    totalNumContentChanges += lifetime.numContentChanges
    maxNumContentChanges += lifetime.numResolves - 1

    breakCounts.append(lifetime.numBreaks)
    contentChangeCounts.append(lifetime.numContentChanges)

    if lifetime.numBreaks == 0:
        numResponsive += 1

    if lifetime.numContents == 1:
        numStable += 1

    if lifetime.numBreaks == 0 and lifetime.numContents == 1:
        numReliable += 1

    if lifetime.lastKnownStatus in (Status.BECAME_UNRESPONSIVE, Status.STILL_UNRESPONSIVE):
        numUnresponsive += 1

    firstUnreliable = numCrawls
    if lifetime.firstResponsePosition is not None:
        crawlFirstResponseTotals[lifetime.firstResponsePosition] += 1
    
    if lifetime.firstBreakPosition is not None:
        crawlFirstBreakTotals[lifetime.firstBreakPosition] += 1
        firstUnreliable = lifetime.firstBreakPosition

    if lifetime.firstChangePosition is not None:
        crawlFirstChangeTotals[lifetime.firstChangePosition] += 1
        firstUnreliable = min(lifetime.firstChangePosition, firstUnreliable)
    
    if firstUnreliable < numCrawls:
        crawlFirstUnreliableTotals[firstUnreliable] += 1

    # If the URL stopped being queried, find out when
    for i, status in enumerate(lifetime.statuses[::-1]):
        if status != Status.UNKNOWN:
            break

    if i > 0:
        crawlAbandonedTotals[numCrawls - i] += 1
        numAbandoned += 1

for _, lifetime in contentLifetimes.items():
    #if lifetime.firstCrawlPosition:
    crawlDatasetTotals[lifetime.firstCrawlPosition] += 1

reallyBads = list()
for url, lifetime in urlLifetimes.items():
    skip = False
    for status in lifetime.statuses:
        if status not in (Status.BECAME_UNRESPONSIVE, Status.STILL_UNRESPONSIVE, Status.UNKNOWN):
            skip = True
            break
    if not skip:
        reallyBads.append((url, lifetime))
numNeverResponded = len(reallyBads)
numResponded = totalNumUrls - numNeverResponded

numUnreliable = totalNumUrls - numReliable
numUnstable = totalNumUrls - numStable
numUnresponsive = totalNumUrls - numResponsive

np.save(outputPath + "totals", {
    "totalNumUrls" : totalNumUrls,
    "totalNumResolves" : totalNumResolves,
    "totalNumBreaks" : totalNumBreaks,
    "totalNumContents" : totalNumContents,
    "totalNumContentChanges" : totalNumContentChanges,
    "maxNumContentChanges" : maxNumContentChanges,
    "numNeverResponded" : numNeverResponded,
    "numUnreliable" : numUnreliable,
    "numUnstable" : numUnstable,
    "numUnresponsive" : numUnresponsive,
})


# In[99]:


textReport = ""

# Totals
textReport += ("Of all %s observed URLs,\n"
    % ("{0:,}".format(totalNumUrls)))
textReport += ("\t%s of URLs (%s total) were responsive\n"
    % ("{0:.2%}".format(numResponsive / totalNumUrls), "{0:,}".format(numResponsive)))
textReport += ("\t%s of URLs (%s total) were stable\n"
    % ("{0:.2%}".format(numStable / numResponded), "{0:,}".format(numStable)))
textReport += ("\t%s of URLs (%s total) were reliable (both responsive and stable)\n"
    % ("{0:.2%}".format(numReliable / totalNumUrls), "{0:,}".format(numReliable)))
textReport += ("\t%s of URLs (%s total) were responsive in the last crawl\n"
    % ("{0:.2%}".format(numUnresponsive / totalNumUrls), "{0:,}".format(numUnresponsive)))
textReport += ("\t%s of URLs (%s total) never responded in any crawl\n"
    % ("{0:.2%}".format(numNeverResponded / totalNumUrls), "{0:,}".format(numNeverResponded)))
textReport += ("\t%s of URLs (%s total) were abandoned\n"
    % ("{0:.2%}".format(numAbandoned / totalNumUrls), "{0:,}".format(numAbandoned)))

# Unreliable URLs
textReport += "\n"
textReport += ("Of the %s unreliable URLs,\n"
    % ("{0:,}".format(numUnreliable)))
textReport += ("\t%s of unreliable URLs (%s total) were not always responsive\n"
    % ("{0:.2%}".format(numUnresponsive / numUnreliable), "{0:,}".format(numUnresponsive)))
textReport += ("\t%s of unreliable URLs (%s total) were not always stable\n"
    % ("{0:.2%}".format(numUnstable / numUnreliable), "{0:,}".format(numUnstable)))

# Behavior
textReport += "\n"
textReport += ("URLs break %s of the time between queries\n"
    % ("{0:.2%}".format(totalNumBreaks / (totalNumResolves + totalNumBreaks))))
textReport += ("URLs contents change %s of the time between queries\n"
    % ("{0:.2%}".format(totalNumContentChanges / maxNumContentChanges)))

file = open(outputPath + "report.txt", "w+")
file.write(textReport)
file.close()

print(textReport)


# In[100]:


breakCountFrequencies = np.unique(breakCounts, return_counts=True)
np.save(outputPath + "url-break-frequencies.npy", breakCountFrequencies)


# In[101]:


rc("text", usetex=True)
rc("savefig", format="pdf", dpi=figureDpi)
rc("font", size=14, family="serif", serif="Computer Modern")
# rc('font', **{'family' : 'serif', 'serif' : ['Computer Modern Roman']})


# In[102]:


if INTERACTIVE:
    figureTitle = observatoryName + ": Frequency Distribution of Total Losses of Responsiveness Per URL"
    outputFile = outputPath + "url-break-freq-dist"

    plt.plot(
        breakCountFrequencies[0],
        breakCountFrequencies[1],
        "-o",
        color="black"
    );
    ax = plt.gca()

    plt.xlabel("Number of losses of responsiveness");
    plt.ylabel("Number of URLs");

    ax.set_yticklabels(["{:,}k".format(int(y / 1000)) for y in plt.yticks()[0]]);

    # Bare
    plt.savefig(outputFile, dpi=figureDpi);

    # Annotated
    plt.title(figureTitle)
    plt.savefig(outputFile + "-annotated", dpi=figureDpi);


# In[103]:


contentChangeCountFrequencies = np.unique(contentChangeCounts, return_counts=True)
np.save(outputPath + "url-content-change-frequencies.npy", contentChangeCountFrequencies)


# In[104]:


if INTERACTIVE:
    figureTitle = observatoryName + ": Frequency Distribution of Total Content Changes Per URL"
    outputFile = outputPath + "url-content-change-freq-dist"

    plt.plot(
        contentChangeCountFrequencies[0],
        contentChangeCountFrequencies[1],
        "-o",
        color="black"
    );
    ax = plt.gca()

    plt.xlabel("Number of content changes");
    plt.ylabel("Number of URLs");

    ax.set_yticklabels(["{:,}k".format(int(y / 1000)) for y in plt.yticks()[0]]);

    # Bare
    plt.savefig(outputFile, dpi=figureDpi);

    # Annotated
    plt.title(figureTitle)
    plt.savefig(outputFile + "-annotated", dpi=figureDpi);


# In[105]:


maxCount = max(len(breakCountFrequencies[0]), len(contentChangeCountFrequencies[0]))

frequencies_df = pd.DataFrame(
    columns=["Unresolvable", "Changed Content"],
    index=range(maxCount),
    data=0
)

for i, numBreaks in enumerate(breakCountFrequencies[0]):
    numUrls = breakCountFrequencies[1][i]
    frequencies_df["Unresolvable"][numBreaks] = numUrls

for i, numChanges in enumerate(contentChangeCountFrequencies[0]):
    numUrls = contentChangeCountFrequencies[1][i]
    frequencies_df["Changed Content"][numChanges] = numUrls

frequencies_df


# In[106]:


figureTitle = observatoryName + ": Frequency Distribution of Breaks and Changes"
outputFile = outputPath + "url-behavior-freq-dist"

ax = frequencies_df.plot(
    color="black",
    style=["-+", "--o"],
    legend=False
);

plt.ylabel("Number of URLs");
plt.xlabel("Number of queries");

ax.set_yticklabels(["{:,}k".format(int(y / 1000)) for y in plt.yticks()[0]]);

# Bare
plt.savefig(outputFile, dpi=figureDpi);

# Annotated
plt.title(figureTitle)
plt.legend()
plt.savefig(outputFile + "-annotated", dpi=figureDpi);


# In[111]:


crawl_totals_df = pd.DataFrame(
    index   = [datetime.datetime.strptime(str(x[1]), "\"%Y-%m-%dT%H:%M:%S.%fZ\"^^<http://www.w3.org/2001/XMLSchema#dateTime>") for x in crawlDates],
)

crawl_totals_df["New URLs"] = crawlUrlTotals
crawl_totals_df["New Contents"] = crawlDatasetTotals
crawl_totals_df["Abandoned"] = crawlAbandonedTotals
crawl_totals_df["First Response"] = crawlFirstResponseTotals
crawl_totals_df["First Break"] = crawlFirstBreakTotals
crawl_totals_df["First Change"] = crawlFirstChangeTotals
crawl_totals_df["First Unreliable"] = crawlFirstUnreliableTotals
crawl_totals_df["Total URLs"] = crawl_totals_df["New URLs"].cumsum()
crawl_totals_df["Total Contents"] = crawl_totals_df["New Contents"].cumsum()
crawl_totals_df["Total Abandoned"] = crawl_totals_df["Abandoned"].cumsum()
crawl_totals_df["Total Responded"] = crawl_totals_df["First Response"].cumsum()
crawl_totals_df["Total Intermittent"] = crawl_totals_df["First Break"].cumsum()
crawl_totals_df["Total Unstable"] = crawl_totals_df["First Change"].cumsum()
crawl_totals_df["Total Unreliable"] = crawl_totals_df["First Unreliable"].cumsum()
crawl_totals_df["Percent Responsive"] = 1 - crawl_totals_df["Total Intermittent"] / crawl_totals_df["Total URLs"]
crawl_totals_df["Percent Stable"] = 1 - crawl_totals_df["Total Unstable"] / crawl_totals_df["Total Responded"]
crawl_totals_df["Percent Reliable"] = 1 - crawl_totals_df["Total Unreliable"] / crawl_totals_df["Total URLs"]

crawl_totals_df


# In[112]:


timeFrame = [
    datetime.datetime.strptime(startDate, "\"%Y-%m-%d"),
    datetime.datetime.strptime(endDate, "\"%Y-%m-%d")
]


# In[113]:


figureTitle = observatoryName + ": Running Total of Unique URLs and Contents"
outputFile = outputPath + "running-total-urls-and-contents"

df = crawl_totals_df[[
    "Total URLs",
    "Total Contents",
#     "Total Abandoned"
]]

ax = df.plot(
    color="black",
    style=["--", ":", "-"],
    legend=False,
    fillstyle="none",
    markersize=7
);
for i, line in enumerate(ax.get_lines()):
    line.set_marker(["s", "D"][i])

plt.xlim(timeFrame);
plt.ylim([0, max(df.max()) * 1.05])

ax.set_yticklabels(["{:,}k".format(int(y / 1000)) for y in plt.yticks()[0]]);

# Bare
plt.savefig(outputFile, dpi=figureDpi);

# Annotated
plt.title(figureTitle)
plt.legend()
plt.savefig(outputFile + "-annotated", dpi=figureDpi);

# Export legend (https://stackoverflow.com/a/50279532)
figsize = (2.2, .75)
fig_leg = plt.figure(figsize=figsize)
ax_leg = fig_leg.add_subplot(111)
# add the legend from the previous axes
ax_leg.legend(*ax.get_legend_handles_labels(), loc='center')
# hide the axes frame and the x/y labels
ax_leg.axis('off')
fig_leg.savefig(outputFile + "-legend")


# In[114]:


figureTitle = observatoryName + ": Responsiveness, Stability, Reliability Over Time"
outputFile = outputPath + "reliability-over-time"

ax = crawl_totals_df[[
    "Percent Responsive",
    "Percent Stable",
    "Percent Reliable"
]].plot(
    color="black",
    style=["--", ":", "-"],
    legend=False,
    fillstyle="none",
    markersize=8
);
for i, line in enumerate(ax.get_lines()):
    line.set_marker(["x", "+", "o"][i])

plt.xlim(timeFrame);
plt.ylim([.25, 1.05]);
ax.set_yticklabels(["{:.0%}".format(float(y)) for y in plt.yticks()[0]]);

# Bare
plt.savefig(outputFile, dpi=figureDpi);

# Annotated
plt.title(figureTitle)
leg = plt.legend()
plt.savefig(outputFile + "-annotated");

# Export legend (https://stackoverflow.com/a/50279532)
figsize = (2.5, 1)
fig_leg = plt.figure(figsize=figsize)
ax_leg = fig_leg.add_subplot(111)
# add the legend from the previous axes
ax_leg.legend(*ax.get_legend_handles_labels(), loc='center')
# hide the axes frame and the x/y labels
ax_leg.axis('off')
fig_leg.savefig(outputFile + "-legend")


# In[115]:


print("Done!")

